# this is just a try
data(cars)
plot(cars)
# this is just a try
data(cars)
plot(cars)
for(i = 1:dim(cars)[1]){
for(i in 1:dim(cars)[1]){
print(i)
}
setwd("~/Documents/DATA/POLITECNICO/PHD/DIDATTICA/StatAppGes1SEM2223/Laboratori/7_HiddenMarkovModels_and_MarkovModels")
#####################################################
###### 7.1.Hidden Markov models [HMM package]  ######
#####################################################
library(HMM)
# How to initialize our HMM
?initHMM
# Initialise HMM nr.1
hmm1 = initHMM(c("X","Y"),               # States: Vector with the names of the states
c("a","b","c")            # Symbols: Vector with the names of the symbols.
)
hmm1
matrix(c(.9,.1,.1,.9),2)
# Initialise HMM nr.2
hmm2 = initHMM(c("X","Y"),               # States: Vector with the names of the states
c("a","b"),               # Symbols: Vector with the names of the symbols.
c(.3,.7),                 # startProbs: Vector with the starting probabilities of the states.
matrix(c(.9,.1,.1,.9),2), # transProbs: Stochastic matrix containing the transition probabilities between the states.
matrix(c(.3,.7,.7,.3),2)  # emissionProbs: Stochastic matrix containing the emission probabilities of the states.
)
hmm2
# Initial HMM
hmm3 = initHMM(c("A","B"),
c("L","R"),
transProbs = matrix(c(.9,.1,.1,.9),2),
emissionProbs = matrix(c(.5,.51,.5,.49),2)
)
# Simulate from the HMM
# Simulates a path of states and observations for a given Hidden Markov Model.
simHMM(hmm3, 100)
# Sequence of observation
a = sample(c(rep("L",100), rep("R",300)))
b = sample(c(rep("L",300), rep("R",100)))
observation = c(a,b)
observation
# Viterbi
?viterbi
viterbi(hmm, observation)
viterbi(hmm3, observation)
# Baum-Welch
?baumWelch
bw = baumWelch(hmm3, observation, 10)
print(bw$hmm)
bw = baumWelch(hmm3, observation, 100)
print(bw$hmm)
# Posterior probabilities for the states
# This function computes the posterior probabilities of being in state X at time k
# for a given sequence of observations and a given Hidden Markov Model.
?posterior
posterior(hmm3, observation)
# Posterior probabilities for the states
# This function computes the posterior probabilities of being in state X at time k
# for a given sequence of observations and a given Hidden Markov Model.
?posterior
posterior(hmm3, observation)
States = c('Fair', 'Loaded')
Symbols = c(1, 2, 3, 4, 5, 6)
observations = c(3, 2, 2, 1, 2, 3, 6, 6, 6, 6)
transProbs = matrix( c(0.95, 0.1, 0.05,  0.9), 2)
transProbs
emissProbs = matrix( c(1/6, 1/10,
1/6, 1/10,
1/6, 1/10,
1/6, 1/10,
1/6, 1/10,
1/6, 1/2 ), 2)
emissProbs
hmm = initHMM(States, Symbols, c(0.5, 0.5), transProbs, emissProbs)
hmm
viterbi(hmm, observations)
print(viterbi(hmm, observations))
# Dishonest casino example
dishonestCasino()
States = c('GC','AT')
Symbols = c('A','C','G','T')
startProbs = c(0.5,0.5)
transProbs = matrix(c(0.80, 0.20,
0.75, 0.25), nrow=2, ncol=2, byrow=TRUE)
transProbs
emissionProbs = matrix(c(0.10,0.40,
0.40,0.10,
0.40,0.10,
0.10,0.40), nrow=4, ncol=2,byrow=TRUE)
emissionProbs
observation = c('C','A','G','C','C','C','T','A','G','T','T','G','C','C','C','C','C','A',
'G','A','G','G','C','A','G','G','T','A','A','A','T','A','G','C','C','A')
hmm = initHMM(States, Symbols, startProbs, transProbs, emissionProbs)
HMM = baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
?baumWelch
HMM
# estimation of E from the observed sequence
HMM$hmm$transProbs
# estimation of T from the observed sequence
HMM$hmm$emissionProbs
# estimation of E from the observed sequence
HMM$hmm$emissionProbs
# estimation of T from the observed sequence
HMM$hmm$transProbs
# The inferred HMM is used for the next tasks.
?viterbiTraining
## Point b
# What is the most probable path for generating the observed sequence, given the estimated E and T ?
# The Viterbi-algorithm computes the most probable
# path of states for a sequence of observations for a given Hidden Markov Model.
viterbi(HMM$hmm, observation)
## Point c
# For each position i in the sequence,
# what are the posterior probabilities of P(Yi = GC|X) and P(Yi =AT|X)
posterior(HMM$hmm, observation)
detach("package:HMM", unload = TRUE)
library(depmixS4)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(dplyr)
library(quantmod)
library(xts)
library(zoo)
library(nnet)
library(MASS)
library(Rsolnp)
library(nlme)
library(TTR)
# this creates data with a single change point with Poisson data
set.seed(3)
y1 <- rpois(50,1)
y2 <- rpois(50,2)
?rpois
ydf <- data.frame(y=c(y1,y2))
View(ydf)
# fit models with 1 to 3 states
m1 <- depmix(y~1, ns=1, family=poisson(), data=ydf)
set.seed(1)
fm1 <- fit(m1)
fm1
m2 <- depmix(y~1, ns=2, family=poisson(), data=ydf)
set.seed(1)
fm2 <- fit(m2)
m3 <- depmix(y~1, ns=3, family=poisson(), data=ydf)
set.seed(1)
fm3 <- fit(m3,em=em.control(maxit=500))
# plot the AICs to select the proper model
plot(1:3,c(AIC(fm1),AIC(fm2),AIC(fm3)),ty="b")
post_probs2 <- posterior(fm2, type = 'viterbi')
post_probs2
layout(1:2)
plot(post_probs2$state, type='s', main='True Regimes', xlab='', ylab='Regime')
matplot(post_probs2[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
post_probs3 <- posterior(fm3, type = 'viterbi')
post_probs3
layout(1:2)
plot(post_probs3$state, type='s', main='True Regimes', xlab='', ylab='Regime')
matplot(post_probs3[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
sample(1:6, 10, replace = T) + sample(1:6, 10, replace = T)
# the setup function: "simulate"
simulate <- function(N, dice.val = 6, jbns, switch.val = 4){
# dice.val is the maximum number of the die
# switch.val is the value strictly below which the die is passed
# jbns is vector of the averages of the jelly beans taken by Bob & Alice
# simulate variables
bob.dice <- sample(1:dice.val, N, replace = T) + sample(1:dice.val, N, replace = T)
alice.dice <- sample(1:dice.val, N, replace = T) + sample(1:dice.val, N, replace = T)
# for the extraction from the jar of jelly beans
bob.jbns <- rpois(N, jbns[1])   # jbns[1] is the average of the poisson for Bob
alice.jbns <- rpois(N, jbns[2]) # jbns[2] is the average of the poisson for Alice
# states
draws <- data.frame(state = rep(NA, N), obs = rep(NA, N), dice = rep(NA, N))
draws$state[1] <- "alice"
draws$obs <- alice.jbns[1]
draws$dice <- alice.dice[1]
for(k in 2:N){
if(draws$state[k-1] == "alice"){
if(draws$dice[k-1] < switch.val+1){
draws$state[k] <- "bob"
draws$obs[k] <- bob.jbns[k]
draws$dice[k] <- bob.dice[k]
}else{
draws$state[k] <- "alice"
draws$obs[k] <- alice.jbns[k]
draws$dice[k] <- alice.dice[k]
}
}else if(draws$state[k-1] == "bob"){
if(draws$dice[k-1] < switch.val+1){
draws$state[k] <- "alice"
draws$obs[k] <- alice.jbns[k]
draws$dice[k] <- alice.dice[k]
}else{
draws$state[k] <- "bob"
draws$obs[k] <- bob.jbns[k]
draws$dice[k] <- bob.dice[k]
}
}
}
# return
return(cbind(roll = 1:N, draws))
}
# simulate scenario
set.seed(20181031)
N <- 100
draws <- simulate(N, jbns = c(12, 4), switch.val = 4)
View(draws)
# observe results
mycols <- c("darkmagenta", "turquoise")
cols <- ifelse(draws$state == "alice", mycols[1], mycols[3])
ggplot(draws, aes(x = roll, y = obs)) + geom_line()
fit.hmm <- function(draws){
# HMM with depmix
mod <- depmix(obs ~ 1, data = draws, nstates = 2, family = poisson())
# use gaussian() for normally distributed data
fit.mod <- fit(mod)
# predict the states by estimating the posterior
est.states <- posterior(fit.mod, type = "viterbi")
print(head(est.states))
# results
tbl <- table(est.states$state, draws$state)
draws$est.state.labels <- c(colnames(tbl)[which.max(tbl[1,])], colnames(tbl)[which.max(tbl[2,])])[est.states$state]
est.states$roll <- 1:N
colnames(est.states)[2:3] <- c(colnames(tbl)[which.max(tbl[1,])], colnames(tbl)[which.max(tbl[2,])])
hmm.post.df <- melt(est.states, measure.vars = c("alice", "bob"))
# print the table
print(table(draws[,c("state", "est.state.labels")]))
# return it
return(list(draws = draws, hmm.post.df = hmm.post.df))
}
hmm1 <- fit.hmm(draws)
head(hmm1$hmm.post.df)
# plot output
plot.hmm.output <- function(model.output){
model.output$draws$state_num = ifelse(model.output$draws$state == 'alice', 1, 0)
model.output$draws$est.state.labels_num = ifelse(model.output$draws$est.state.labels == 'alice', 1, 0)
g0 <- (ggplot(model.output$draws, aes(x = roll, y = obs)) + geom_line() +
theme(axis.ticks = element_blank(), axis.title.y = element_blank())) %>% ggplotGrob
g1 <- (ggplot(model.output$draws, aes(x = roll, y = state_num, colour = factor(state_num))) +
geom_point() +
theme(axis.ticks = element_blank(), axis.text.y = element_blank()) +
labs(y = "Actual State", colour = "State:\nPerson that\nrolled the\ndice") +
scale_color_manual(labels = c("bob", "alice"), values=c("turquoise", "darkmagenta"))) %>% ggplotGrob
g2 <- (ggplot(model.output$draws, aes(x = roll, y = est.state.labels_num, colour = factor(est.state.labels_num))) +
geom_point() +
theme(axis.ticks = element_blank(), axis.text.y = element_blank()) +
labs(y = "Estimated State", colour = "State:\nPerson that\nrolled the\ndice") +
scale_color_manual(labels = c("bob", "alice"), values=c("turquoise", "darkmagenta"))) %>% ggplotGrob
g3 <- (ggplot(model.output$hmm.post.df, aes(x = roll, y = value, col = variable)) + geom_line() +
scale_color_manual(values = mycols, name = "State:\nPerson that\nrolled the\ndice", labels = c("Alice", "Bob")) +
theme(axis.ticks = element_blank(), axis.text.y = element_blank()) +
labs(y = "Posterior Prob.")) %>% ggplotGrob()
g0$widths <- g1$widths
return(grid.arrange(g0, g1, g2, g3, widths = 1, nrow = 4))
}
plot.hmm.output(hmm1)
# What if the transition probabilities were much greater?
# we set switch.val = 7
draws <- simulate(100, jbns = c(12, 4), switch.val = 7)
cols <- ifelse(draws$state == "alice", mycols[1], mycols[3])
hmm2 <- fit.hmm(draws)
plot.hmm.output(hmm2)
# What if the observations are drawn from the same distribution
# i.e. Alice and Bob take the same amount of jelly beans?
draws <- simulate(100, jbns = c(12, 13), switch.val = 4)
cols <- ifelse(draws$state == "alice", mycols[1], mycols[3])
hmm3 <- fit.hmm(draws)
plot.hmm.output(hmm3)
# What if the observations are drawn from the same distribution
# i.e. Alice and Bob take the same amount of jelly beans?
draws <- simulate(100, jbns = c(12, 12), switch.val = 4)
cols <- ifelse(draws$state == "alice", mycols[1], mycols[3])
hmm3 <- fit.hmm(draws)
plot.hmm.output(hmm3)
set.seed(1)
# Create the parameters for the bull and
# bear market returns distributions
Nk_lower <- 50
Nk_upper <- 150
bull_mean <- 0.1
bull_var <- 0.1
bear_mean <- -0.05
bear_var <- 0.2
# Create the list of durations (in days) for each regime
days <- replicate(5, sample(Nk_lower:Nk_upper, 1))
days
market_bull_1 <- rnorm( days[1], bull_mean, bull_var )  # BULL
market_bear_2 <- rnorm( days[2], bear_mean, bear_var )  # BEAR
market_bull_3 <- rnorm( days[3], bull_mean, bull_var )  # BULL
market_bear_4 <- rnorm( days[4], bear_mean, bear_var )  # BEAR
market_bull_5 <- rnorm( days[5], bull_mean, bull_var )  # BULL
# Create the list of true regime states and full returns list
# 1 stands for BULL and 2 stands for BEAR
true_regimes <- c(rep(1,days[1]), rep(2,days[2]), rep(1,days[3]), rep(2,days[4]), rep(1,days[5]))
true_regimes
returns <- c(market_bull_1, market_bear_2, market_bull_3, market_bear_4, market_bull_5)
returns
plot(returns, type="l", xlab='', ylab="Returns")
# Create and fit the Hidden Markov Model
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
hmmfit <- fit(hmm, verbose = FALSE)
# Output both the true regimes and the
# posterior probabilities of the regimes
post_probs <- posterior(hmmfit, type = 'viterbi')
post_probs
layout(1:2)
plot(post_probs$state, type='s', main='True Regimes', xlab='', ylab='Regime')
matplot(post_probs[,-1], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='topright', c('Bull','Bear'), fill=1:2, bty='n')
graphics.off()
# Obtain S&P500 data from 2004 onwards and create the returns stream from this
getSymbols("^GSPC", from="2004-01-01")
chartSeries(GSPC, theme='black')
View(GSPC)
?Cl
getSymbols("^GSPC", from="2004-01-01")
chartSeries(GSPC, theme='black')
gspcRets = diff( log( Cl( GSPC ) ) )
?Cl
gspcRets = gspcRets[-1,]
View(gspcRets)
gspcRets = gspcRets[-1,]
returns = as.numeric(gspcRets)
plot(gspcRets)
# Fit a Hidden Markov Model with two states
# to the S&P500 returns stream
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 2, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
summary(hmmfit)
post_probs <- posterior(hmmfit, type='viterbi')
layout(1:2)
post_probs$date = index(gspcRets)
plot(x = post_probs$date, y = returns, type='l', main='Regime Detection', xlab='', ylab='Returns')
matplot(x = post_probs$date, y = post_probs[,c(2,3)], type='l', main='Regime Posterior Probabilities', ylab='Probability')
legend(x='bottomleft', c('Regime #1','Regime #2'), fill=1:2, bty='n')
GSPC_Predict = post_probs$state
chartSeries(GSPC)
addTA(GSPC[GSPC_Predict==1,1], on=1, type='p', col=5, pch=10)
addTA(GSPC[GSPC_Predict==2,1], on=1, type='p', col=6, pch=25)
graphics.off()
# The same process will now be carried out for a three-state HMM:
# Fit a Hidden Markov Model with three states
# to the S&P500 returns stream
hmm <- depmix(returns ~ 1, family = gaussian(), nstates = 3, data=data.frame(returns=returns))
hmmfit <- fit(hmm, verbose = FALSE)
summary(hmmfit)
post_probs <- posterior(hmmfit, type='viterbi')
layout(1:2)
post_probs$date = index(gspcRets)
plot(x = post_probs$date, y = returns, type = 'l', main = 'Regime Detection', xlab = '', ylab = 'Returns')
matplot(x = post_probs$date, y = post_probs[,c(2,3,4)], type = 'l', main = 'Regime Posterior Probabilities', ylab = 'Probability')
legend(x='topright', c('Regime #1','Regime #2', 'Regime #3'), fill = 1:3, bty = 'n', cex = 0.75)
GSPC_Predict = post_probs$state
chartSeries(GSPC)
addTA(GSPC[GSPC_Predict==1,1], on=1, type='p', col=5, pch=10)
addTA(GSPC[GSPC_Predict==2,1], on=1, type='p', col=6, pch=25)
addTA(GSPC[GSPC_Predict==3,1], on=1, type='p', col=7, pch=24)
detach("package:depmixS4", unload = TRUE)
# Create a markovchain object
mcWeather <- new("markovchain",
# states: a character vector, listing the states for which transition probabilities are defined.
states = c("sunny", "cloudy", "rain"),
# transitionMatrix: the probabilities of the transition matrix.
transitionMatrix = matrix(data = c(0.70, 0.2, 0.1,
0.3, 0.4, 0.3,
0.2, 0.45, 0.35), byrow = TRUE, nrow = 3),
# name: optional character element to name the DTMC
name = "Weather")
library(markovchain)
library(igraph)
# Create a markovchain object
mcWeather <- new("markovchain",
# states: a character vector, listing the states for which transition probabilities are defined.
states = c("sunny", "cloudy", "rain"),
# transitionMatrix: the probabilities of the transition matrix.
transitionMatrix = matrix(data = c(0.70, 0.2, 0.1,
0.3, 0.4, 0.3,
0.2, 0.45, 0.35), byrow = TRUE, nrow = 3),
# name: optional character element to name the DTMC
name = "Weather")
initialState <- c(0, 1, 0)
after2Days <- initialState * (mcWeather * mcWeather)
after7Days <- initialState * (mcWeather ^ 7)
after2Days
round(after7Days, 3)
print(mcWeather)
plot(mcWeather)
conditionalDistribution(mcWeather, "sunny")
steadyStates(mcWeather)
# Absorbing states are determined by means of absorbingStates method.
absorbingStates(mcWeather)  # no absorbing states
summary(mcWeather)  ##classification of states
is.accessible(object = mcWeather, from = "sunny", to = "rain")
is.irreducible(mcWeather)
period(mcWeather)
IrrMatr <- matrix(0,nrow = 5, ncol = 5)
IrrMatr[1,] <- c(0, 1/3, 0, 2/3, 0)
IrrMatr[2,] <- c(1/2, 0, 0, 0, 1/2)
IrrMatr[3,] <- c(0, 0, 1/2, 1/2, 0)
IrrMatr[4,] <- c(0, 0, 1/2, 1/2, 0)
IrrMatr[5,] <- c(0, 0, 0, 0, 1)
statesNames <- letters[1:5]
IrrMc <- new("markovchain", transitionMatrix = IrrMatr, name = "Irreducible MC", states = statesNames)
summary(IrrMc)
IrrMatr
IrrMatr <- matrix(0,nrow = 5, ncol = 5)
IrrMatr[1,] <- c(0, 1/3, 0, 2/3, 0)
IrrMatr[2,] <- c(1/2, 0, 0, 0, 1/2)
IrrMatr[3,] <- c(0, 0, 1/2, 1/2, 0)
IrrMatr[4,] <- c(0, 0, 1/2, 1/2, 0)
IrrMatr[5,] <- c(0, 0, 0, 0, 1)
statesNames <- letters[1:5]
IrrMatr
IrrMc <- new("markovchain",
transitionMatrix = IrrMatr,
name = "Irreducible MC",
states = statesNames)
summary(IrrMc)
## Gambler Ruin example
# It is possible for a Markov chain to have more than one stationary distribution,
# as the gambler ruin example shows.
gamblerRuinMarkovChain <- function(moneyMax, prob = 0.5) {
m <- matrix(0,nrow = moneyMax + 1, ncol = moneyMax + 1)
m[1,1] <- m[moneyMax + 1,moneyMax + 1] <- 1
states <- as.character(0:moneyMax)
rownames(m) <- colnames(m) <- states
for(i in 2:moneyMax){
m[i,i-1] <- 1 - prob
m[i, i + 1] <- prob
}
new("markovchain",
transitionMatrix = m,
name = paste("Gambler ruin", moneyMax, "dim", sep = " ")
)
}
mcGR4 <- gamblerRuinMarkovChain(moneyMax = 4, prob = 0.5)
steadyStates(mcGR4)
plot(mcGR4)
summary(mcGR4)
weathersOfDays <- rmarkovchain(n = 365, object = mcWeather, t0 = "sunny")
weathersOfDays[1:30]
weatherFittedMLE <- markovchainFit(data = weathersOfDays, method = "mle", name = "Weather")
weatherFittedMLE$estimate
weatherFittedMLE$standardError
predict(object = weatherFittedMLE$estimate, newdata = c("cloudy", "sunny"), n.ahead = 3)
# The verifyMarkovProperty function verifies whether the Markov property holds for the given chain.
verifyMarkovProperty(weathersOfDays)
# Another artificial example
sample_sequence <- c("a", "b", "a", "a", "a", "a", "b", "a", "b", "a", "b", "a", "a", "b", "b", "b", "a")
verifyMarkovProperty(sample_sequence)
